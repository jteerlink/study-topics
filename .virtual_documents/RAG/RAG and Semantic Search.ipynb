# Core tools
!pip install faiss-cpu sentence-transformers pypdf nltk tqdm requests chromadb openai streamlit




!python -m nltk.downloader punkt


# Ollama: Install & pull model
brew install ollama  # or use curl install for Linux/Windows
ollama pull llama3


from pathlib import Path
import re

# Load the file
file_path = "11nelson.txt"
raw_text = Path(file_path).read_text(encoding="utf-8", errors="ignore")

# Normalize: remove funky characters and clean whitespace
def clean_text(text):
    # Fix weird characters (like â from encoding errors)
    text = re.sub(r'[^\x00-\x7F]+', ' ', text)
    # Collapse newlines
    text = re.sub(r'\n+', '\n', text)
    # Remove extra spaces
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

cleaned_text = clean_text(raw_text)
print(cleaned_text[:1000])  # preview cleaned output





def chunk_fixed(text, chunk_size=400, overlap=50):
    words = text.split()
    chunks = []
    for i in range(0, len(words), chunk_size - overlap):
        chunk = " ".join(words[i:i + chunk_size])
        chunks.append(chunk)
    return chunks

chunks = chunk_fixed(cleaned_text)
print(f"Chunk count: {len(chunks)}")
print(f"First chunk:\n{chunks[0][:500]}")


import nltk
nltk.download('punkt')
nltk.download('punkt_tab')
from nltk.tokenize import sent_tokenize

def chunk_sentences(text, max_words=400):
    sentences = sent_tokenize(text)
    chunks = []
    current_chunk = []

    current_len = 0
    for sent in sentences:
        word_count = len(sent.split())
        if current_len + word_count <= max_words:
            current_chunk.append(sent)
            current_len += word_count
        else:
            chunks.append(" ".join(current_chunk))
            current_chunk = [sent]
            current_len = word_count
    if current_chunk:
        chunks.append(" ".join(current_chunk))

    return chunks

chunks = chunk_sentences(cleaned_text)
print(f"Chunk count: {len(chunks)}")
print(f"Sample chunk:\n{chunks[0][:500]}")



chunk_data = [
    {
        "chunk_id": i,
        "text": chunk,
        "source": file_path
    }
    for i, chunk in enumerate(chunks)
]



