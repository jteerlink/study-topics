ğŸ“š 1. Lesson Overview

ğŸ¯ Objectives:
By the end of this multi-module lesson, youâ€™ll be able to:
	â€¢	Manually construct each component of a RAG pipeline.
	â€¢	Apply effective document preprocessing and chunking strategies.
	â€¢	Compute and store vector embeddings locally.
	â€¢	Build a semantic retriever using FAISS or Chroma with custom indexing.
	â€¢	Integrate with a local LLM (via ollama) for context-grounded generation.
	â€¢	Evaluate retrieval quality and handle hallucinations.

ğŸ§° Stack:
	â€¢	LLM: Ollama (LLaMA 3 or Mistral)
	â€¢	Embedder: sentence-transformers â†’ all-MiniLM-L6-v2
	â€¢	Vector DB: FAISS (baseline), Chroma (advanced option)
	â€¢	Preprocessing: pypdf, nltk, re
	â€¢	No LangChain or agentic tools â€” weâ€™ll use them after the manual version for contrast

â¸»
Module
Topic
Outcome
1
ğŸ” RAG Architecture Deep Dive
Understand flow: input â†’ retrieve â†’ generate
2
ğŸ“„ Data Ingestion & Preprocessing
Load PDFs/Markdown, clean, normalize
3
âœ‚ï¸ Document Chunking Strategies
Fixed, overlap, semantic
4
ğŸ”¢ Embedding with Sentence Transformers
Generate dense vectors
5
ğŸ—ƒï¸ Vector Database Setup (FAISS)
Store, search, and rank results
6
ğŸ§  LLM Integration with Ollama
Use prompt templates, local inference
7
ğŸ” Putting It All Together
End-to-end RAG pipeline
8
ğŸ“ˆ Evaluation & Extension
RAGEval, hallucination handling, scaling



	â€¢	Precision control over:
	â€¢	how chunks are created
	â€¢	which documents get embedded or excluded
	â€¢	how relevance is scored and re-ranked
	â€¢	RAG vs Fine-tuning: cost, latency, flexibility
	â€¢	Optimizing local RAG for laptop performance
	â€¢	Common failure modes: embedding drift, retrieval miss, hallucination
	â€¢	Modular design: ability to swap out LLMs, retrievers, or chunkers
